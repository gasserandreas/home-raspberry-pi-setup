# Setup Airflow

## 1. Setup system
1. Create 'apache-airflow' data directory under `~/data`: `mkdir ~/data/apache-airflow` 
2. Change root directory to: `docker/apache-airflow`
3. Copy `.env.example` to `.env` file: `cp .env.example .env`
4. Change "AIRFLOW_PROJ_DIR" to: `~/docker-data/apache-airflow`
5. Modify default user & passwords (recommended: change all passwords)
6. Start apache with `docker compose up -d`

## 2. Configure data database
1. go to "Admin" -> "Connections"
2. add new connection:
  - **Connection Id:** value of `airflow_data`
  - **Connection Type:** `Postgres`
  - **Host:** value of Postgres data service, example: `airflow-data` 
  - **Database:** value of `DATA_POSTGRES_DB` in .env file
  - **Login:** value of `DATA_POSTGRES_USER` in .env file
  - **Password:** value of `DATA_POSTGRES_PASSWORD` in .env file
  - **Port:** `5432` (internal Docker network port is required)
3. test database

## 3. Clone Airflow repo
1. cd to Docker data folder: `cd ~/docker-data/apache-airflow`
2. delete existing files & directories in this folder
3. change access rights to allow read/write for all users: `chmod 777 ~/docker-data/apache-airflow`
4. clone "home-airflow-data" repo into folder: `git clone git@github.com:gasserandreas/home-airflow-data.git .` (important: make sure `.` is copied to clone content rather folder)

